{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5HQ7JBCsBBoK",
        "nrp1NRc5A6xo",
        "jBDmAeWJA0KM",
        "Vw6slzQaAvj7",
        "hbn0pf2GAiyA",
        "Lz_KZ7liAd4v",
        "AcLtZftCAY2M",
        "fmHCFh3IASSk",
        "yo3pZ-2lAMoa",
        "rF4qsPt7AGw6",
        "kFmOCt00_-KY",
        "VLVTW37K_1ka",
        "8wmK4Mfv_y_4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**üìù Project Overview**\n",
        "\n",
        "This notebook implements an AI-powered Customer Support Desk capable of understanding user queries, retrieving relevant knowledge base information, generating natural language responses, evaluating quality, and performing escalation when required.\n",
        "The system integrates intent classification, semantic search using FAISS, Gemini 1.5 LLM generation, and a Gradio-based interactive UI."
      ],
      "metadata": {
        "id": "X20PAnv35PqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Library Installation"
      ],
      "metadata": {
        "id": "5HQ7JBCsBBoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù Library Installation and Environment Setup\n",
        "\n",
        "In this step, we prepare the Colab environment by uninstalling older versions of the google-generativeai package and reinstalling the latest stable release.\n",
        "This ensures compatibility with Gemini 1.5 models and prevents deprecated API usage.\n",
        "We also install all supporting dependencies required for embeddings, vector search, and the Gradio interface."
      ],
      "metadata": {
        "id": "d4KNxSOC5dtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y google-generativeai google_ai_genai google-ai-generativelanguage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuY4j5t30qHZ",
        "outputId": "7d11a1b4-89a0-4330-c168-a1e0e703edd2"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: google-generativeai 0.8.5\n",
            "Uninstalling google-generativeai-0.8.5:\n",
            "  Successfully uninstalled google-generativeai-0.8.5\n",
            "\u001b[33mWARNING: Skipping google_ai_genai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: google-ai-generativelanguage 0.6.15\n",
            "Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "  Successfully uninstalled google-ai-generativelanguage-0.6.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "H_ObbvmU0ty_",
        "outputId": "1c2b4251-669e-4219-c6ba-c0e5d4920ecb"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
            "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.10.5)\n",
            "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-ai-generativelanguage, google-generativeai\n",
            "Successfully installed google-ai-generativelanguage-0.6.15 google-generativeai-0.8.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "0d164e8d34de45888dbf2d2ace0e7bc4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLzYxyeF0vqp",
        "outputId": "92a953db-c1f4-4c94-c811-77f2acc49bfb"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: google-generativeai\n",
            "Version: 0.8.5\n",
            "Summary: Google Generative AI High level API client library and tools.\n",
            "Home-page: https://github.com/google/generative-ai-python\n",
            "Author: Google LLC\n",
            "Author-email: googleapis-packages@google.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: google-ai-generativelanguage, google-api-core, google-api-python-client, google-auth, protobuf, pydantic, tqdm, typing-extensions\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "genai.list_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWq_JhmK1HJm",
        "outputId": "6ba6fee4-55d7-48a0-aaea-deed41692cd4"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object list_models at 0x79e6a59388b0>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7H2p1baylfF",
        "outputId": "b2e25233-8389-4cc0-db5a-f71cf3eaa657"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Imports"
      ],
      "metadata": {
        "id": "nrp1NRc5A6xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù Importing Core Dependencies\n",
        "\n",
        "Here, we import all the essential Python libraries required for the project, including modules for:\n",
        "\n",
        "- large language model interaction\n",
        "\n",
        "- vector embedding and FAISS-based retrieval\n",
        "\n",
        "- dataclasses and type definitions\n",
        "\n",
        "- Gradio UI creation\n",
        "\n",
        "- utility functions for text processing\n",
        "\n",
        "These imports lay the foundation for the complete AI customer support pipeline."
      ],
      "metadata": {
        "id": "tCADGa0b5pmo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "ovNcBnhNso_X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import traceback\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
        "import threading\n",
        "import numpy as np\n",
        "import faiss\n",
        "import tiktoken\n",
        "import google.generativeai as genai\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Configuration"
      ],
      "metadata": {
        "id": "jBDmAeWJA0KM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù Global Configuration and API Initialization\n",
        "\n",
        "This section configures core project settings such as the Gemini API key, model parameters, and global constants used across the pipeline.\n",
        "Ensuring proper configuration is essential for seamless communication with the LLM and consistent system behavior."
      ],
      "metadata": {
        "id": "HoDg3M285-t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "if not API_KEY:\n",
        "    print(\"WARNING: GEMINI_API_KEY not set in environment. LLM calls will fail until configured.\")\n",
        "else:\n",
        "    genai.configure(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "DDexdzeSs790"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Utilities"
      ],
      "metadata": {
        "id": "Vw6slzQaAvj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù Utility Functions: Tokenization, Chunking, and Embedding\n",
        "\n",
        "In this section, we implement helper functions for text preprocessing.\n",
        "\n",
        "These utilities handle tasks such as:\n",
        "\n",
        "- breaking documents into manageable chunks\n",
        "\n",
        "- generating vector embeddings\n",
        "\n",
        "- normalizing or preparing text for retrieval\n",
        "\n",
        "These low-level operations are used throughout the retrieval pipeline."
      ],
      "metadata": {
        "id": "Kq4kGKJj6uG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilities: tokenization, chunking, embeddings\n",
        "\n",
        "try:\n",
        "    _default_token_enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "except Exception:\n",
        "    _default_token_enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "\n",
        "def chunk_text(text: str, max_tokens: int = 300) -> List[str]:\n",
        "    enc = _default_token_enc\n",
        "    toks = enc.encode(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(toks), max_tokens):\n",
        "        chunk = enc.decode(toks[i:i + max_tokens])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def embed_texts(text_list: List[str]) -> np.ndarray:\n",
        "    \"\"\"Embed list of texts using Gemini embeddings model 'text-embedding-004'.\n",
        "    Returns float32 matrix of shape (N, D).\n",
        "    \"\"\"\n",
        "    if isinstance(text_list, str):\n",
        "        text_list = [text_list]\n",
        "\n",
        "\n",
        "    embeddings = []\n",
        "    for text in text_list:\n",
        "        try:\n",
        "            resp = genai.embed_content(model=\"text-embedding-004\", content=text)\n",
        "            emb = np.array(resp[\"embedding\"], dtype=\"float32\")\n",
        "        except Exception as e:\n",
        "            # graceful fallback to a random vector so FAISS code still runs in offline testing\n",
        "            print(\"Embedding call failed:\", e)\n",
        "            emb = np.random.rand(768).astype(\"float32\")\n",
        "        embeddings.append(emb)\n",
        "\n",
        "\n",
        "    return np.vstack(embeddings)"
      ],
      "metadata": {
        "id": "jAWNvhNNs87N"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. FAISS Index"
      ],
      "metadata": {
        "id": "hbn0pf2GAiyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù FAISS Index Construction and Helper Methods\n",
        "\n",
        "This cell contains the logic for building and querying the FAISS vector store.\n",
        "\n",
        "FAISS enables efficient similarity search by comparing user query embeddings with knowledge base document embeddings.\n",
        "\n",
        "These helpers support storing vectors, searching for relevant chunks, and managing the vector index."
      ],
      "metadata": {
        "id": "BgsZ9kV263zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FAISS index helpers\n",
        "\n",
        "def build_kb_embeddings(docs: List[str]) -> Tuple[np.ndarray, List[Dict[str, Any]]]:\n",
        "    \"\"\"Take a list of document strings, chunk them, embed them, and return embedding matrix\n",
        "    plus a list of chunk metadata dictionaries: {'id','text','source'}\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "    for i, doc in enumerate(docs):\n",
        "        chunks = chunk_text(doc)\n",
        "        for j, c in enumerate(chunks):\n",
        "            all_chunks.append({\n",
        "                \"id\": f\"doc_{i}_chunk_{j}\",\n",
        "                \"text\": c,\n",
        "                \"source\": f\"doc_{i}\"\n",
        "            })\n",
        "\n",
        "    print(f\"Total chunks: {len(all_chunks)}\")\n",
        "    if len(all_chunks) == 0:\n",
        "        return np.zeros((0, 768), dtype=\"float32\"), []\n",
        "\n",
        "    texts = [c[\"text\"] for c in all_chunks]\n",
        "    emb_matrix = embed_texts(texts)\n",
        "    return emb_matrix, all_chunks\n",
        "\n",
        "\n",
        "def build_faiss_index(emb_matrix: np.ndarray) -> faiss.IndexFlatL2:\n",
        "    if emb_matrix.size == 0:\n",
        "        # create a small dummy index with dimension 768\n",
        "        dim = 768\n",
        "        index = faiss.IndexFlatL2(dim)\n",
        "        return index\n",
        "    dim = emb_matrix.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(emb_matrix)\n",
        "    print(\"FAISS index built with\", index.ntotal, \"vectors\")\n",
        "    return index\n",
        "\n",
        "\n",
        "def search_index(index: faiss.IndexFlatL2, query: str, chunks: List[Dict[str, Any]], k: int = 3) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Return top-k chunk dicts with fields: {id, text, source, distance}\n",
        "\n",
        "\n",
        "    If embedding call fails, returns empty list.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        q_emb = embed_texts([query]).reshape(1, -1)\n",
        "    except Exception as e:\n",
        "        print(\"Query embedding failed:\", e)\n",
        "        return []\n",
        "\n",
        "\n",
        "    if index.ntotal == 0:\n",
        "        return []\n",
        "\n",
        "\n",
        "    distances, indices = index.search(q_emb, k)\n",
        "    results = []\n",
        "    for dist, idx in zip(distances[0], indices[0]):\n",
        "        if idx < 0 or idx >= len(chunks):\n",
        "            continue\n",
        "        chunk = chunks[idx]\n",
        "        out = {\n",
        "            \"id\": chunk.get(\"id\"),\n",
        "            \"text\": chunk.get(\"text\"),\n",
        "            \"source\": chunk.get(\"source\"),\n",
        "            \"distance\": float(dist)\n",
        "        }\n",
        "        results.append(out)\n",
        "    return results"
      ],
      "metadata": {
        "id": "UbSHdpveuJAz"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Agent Base Classes and Tools"
      ],
      "metadata": {
        "id": "Lz_KZ7liAd4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù Agent Base Structure, Shared Tools, and Response Definitions\n",
        "\n",
        "This section defines the foundational components used by all agents in the system.\n",
        "\n",
        "It includes:\n",
        "\n",
        "- BaseAgent class (providing shared functionality)\n",
        "\n",
        "- Structured response dataclasses\n",
        "\n",
        "- Reusable helper tools for logging and processing\n",
        "\n",
        "These abstractions enforce consistent behavior across all specialized agents and simplify the design of the overall pipeline."
      ],
      "metadata": {
        "id": "tzdabNeN93ZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent base, response dataclass, tools\n",
        "\n",
        "@dataclass\n",
        "class AgentResponse:\n",
        "    agent_name: str\n",
        "    success: bool\n",
        "    output: Any = None\n",
        "    error: Optional[str] = None\n",
        "    meta: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "        self._logs: List[Dict[str, Any]] = []\n",
        "        self._lock = threading.Lock()\n",
        "        self.llm: Optional[\"LLMClient\"] = None # make llm attribute explicit\n",
        "\n",
        "    def _log(self, level: str, message: str, **meta):\n",
        "        entry = {\"ts\": time.time(), \"level\": level, \"agent\": self.name, \"message\": message, \"meta\": meta}\n",
        "        with self._lock:\n",
        "            self._logs.append(entry)\n",
        "        print(f\"[{self.name}][{level}] {message}\")\n",
        "\n",
        "    def info(self, message: str, **meta):\n",
        "        self._log(\"INFO\", message, **meta)\n",
        "\n",
        "    def warn(self, message: str, **meta):\n",
        "        self._log(\"WARN\", message, **meta)\n",
        "\n",
        "    def error(self, message: str, **meta):\n",
        "        self._log(\"ERROR\", message, **meta)\n",
        "\n",
        "    def get_logs(self) -> List[Dict[str, Any]]:\n",
        "        return list(self._logs)\n",
        "\n",
        "    def run_safe(self, fn: Callable[..., Any], *args, **kwargs) -> AgentResponse:\n",
        "        start = time.time()\n",
        "        try:\n",
        "            out = fn(*args, **kwargs)\n",
        "            latency = time.time() - start\n",
        "            self.info(\"Call succeeded\", latency=latency)\n",
        "            return AgentResponse(agent_name=self.name, success=True, output=out, meta={\"latency\": latency})\n",
        "        except Exception as e:\n",
        "            latency = time.time() - start\n",
        "            tb = traceback.format_exc()\n",
        "            self.error(\"Call failed\", error=str(e), latency=latency)\n",
        "            return AgentResponse(agent_name=self.name, success=False, error=str(e), meta={\"latency\": latency, \"traceback\": tb})\n",
        "\n",
        "\n",
        "class Tool:\n",
        "    def __init__(self, name: str, fn: Callable[..., Any]):\n",
        "        self.name = name\n",
        "        self.fn = fn\n",
        "\n",
        "    def call(self, *args, **kwargs):\n",
        "        return self.fn(*args, **kwargs)"
      ],
      "metadata": {
        "id": "tgBbUUj2ukCC"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. LLM Client Wrapper"
      ],
      "metadata": {
        "id": "AcLtZftCAY2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù LLM Client Wrapper for Gemini 1.5 Models\n",
        "\n",
        "Here, we implement a dedicated wrapper class around the Google Gemini API.\n",
        "This wrapper standardizes how prompts are sent to the model, manages generation configuration, and handles errors gracefully.\n",
        "\n",
        "By abstracting these operations, downstream agents can easily generate content using a clean interface."
      ],
      "metadata": {
        "id": "4a8_nSwY7HVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM client wrapper (Google Generative AI) - updated default model\n",
        "\n",
        "class LLMClient:\n",
        "    def __init__(self, model=\"gemini-1.5-flash\", temperature=0.0, max_output_tokens=512):\n",
        "        self.model_name = model\n",
        "        self.temperature = temperature\n",
        "        self.max_output_tokens = max_output_tokens\n",
        "\n",
        "        try:\n",
        "            self.model = genai.GenerativeModel(self.model_name)\n",
        "        except Exception as e:\n",
        "            print(\"Failed to initialize LLM model:\", e)\n",
        "            self.model = None\n",
        "\n",
        "    def generate_content(self, prompt: str):\n",
        "        class Resp:\n",
        "            text = \"\"\n",
        "\n",
        "        r = Resp()\n",
        "\n",
        "        if not self.model:\n",
        "            r.text = \"Generation failed: LLM not initialized\"\n",
        "            return r\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(\n",
        "                prompt,   # <-- SIMPLE STRING (new API)\n",
        "                generation_config={\n",
        "                    \"temperature\": self.temperature,\n",
        "                    \"max_output_tokens\": self.max_output_tokens\n",
        "                }\n",
        "            )\n",
        "            r.text = response.text\n",
        "            return r\n",
        "\n",
        "        except Exception as e:\n",
        "            r.text = f\"Generation failed: {e}\"\n",
        "            return r\n"
      ],
      "metadata": {
        "id": "55rXPvQMuvcz"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Agent Implementations"
      ],
      "metadata": {
        "id": "fmHCFh3IASSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù Specialized Agents for Classification, Retrieval, Generation, Evaluation, and Escalation\n",
        "\n",
        "\n",
        "This cell defines the individual agents that perform the core system tasks:\n",
        "\n",
        "- intent classification\n",
        "\n",
        "- semantic retrieval\n",
        "\n",
        "- LLM-based response generation\n",
        "\n",
        "- quality evaluation\n",
        "\n",
        "- escalation decision-making\n",
        "\n",
        "Each agent inherits from the base structure and implements customized behavior for its respective role."
      ],
      "metadata": {
        "id": "7tYjNeNA-S8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent implementations: classifier, retriever, generator, evaluator, escalation\n",
        "\n",
        "class IntentClassifierAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"classifier\")\n",
        "\n",
        "    def classify_intent(self, user_msg: str) -> str:\n",
        "        # For now, a very simple keyword-based classification\n",
        "        if \"sso\" in user_msg.lower() or \"oauth\" in user_msg.lower():\n",
        "            return \"sso_oauth_inquiry\"\n",
        "        elif \"refund\" in user_msg.lower():\n",
        "            return \"refund_request\"\n",
        "        return \"general_inquiry\"\n",
        "\n",
        "class RetrieverAgent(Agent):\n",
        "    def __init__(self, index: faiss.IndexFlatL2, chunks: List[Dict[str, Any]]):\n",
        "        super().__init__(\"retriever\")\n",
        "        self.index = index\n",
        "        self.chunks = chunks\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
        "        resp = self.run_safe(lambda: search_index(self.index, query, self.chunks, k=k))\n",
        "        if resp.success and resp.output:\n",
        "            return resp.output\n",
        "        return []\n",
        "\n",
        "\n",
        "class ResponseGeneratorAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"generator\")\n",
        "\n",
        "    def generate_answer(self, intent: str, user_msg: str, retrieved_chunks: List[Dict[str, Any]]) -> str:\n",
        "        # compose context\n",
        "        context = \"\\n\\n\".join([c.get(\"text\", \"\") for c in retrieved_chunks])\n",
        "        prompt = f\"You are a SaaS support assistant. Intent: {intent}\\nContext:\\n{context}\\nUser message:\\n{user_msg}\\nProvide a concise helpful response.\"\n",
        "\n",
        "        if not self.llm or not self.llm.model:\n",
        "            # fallback canned response\n",
        "            return \"Thanks for reaching out. We support SSO/OAuth integrations; please check your account settings or contact support for setup help.\" if \"sso\" in user_msg.lower() or \"oauth\" in user_msg.lower() else \"Thank you ‚Äî we will look into this and get back to you.\"\n",
        "\n",
        "        resp = self.run_safe(lambda: self.llm.generate_content(prompt).text)\n",
        "        if resp.success and isinstance(resp.output, str):\n",
        "            return resp.output\n",
        "        return \"Generation failed: see agent logs.\"\n",
        "\n",
        "\n",
        "class EvaluationAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"evaluator\")\n",
        "\n",
        "    def evaluate(self, user_msg: str, answer: str) -> int:\n",
        "        if not self.llm or not self.llm.model:\n",
        "            return 3\n",
        "        prompt = f\"Score 1-5 how helpful this answer is for the user.\\nUser:\\n{user_msg}\\nAnswer:\\n{answer}\\nReturn only a single integer.\"\n",
        "        resp = self.run_safe(lambda: self.llm.generate_content(prompt).text)\n",
        "        if not resp.success:\n",
        "            return 3\n",
        "        try:\n",
        "            return int(str(resp.output).strip().split()[0])\n",
        "        except Exception:\n",
        "            return 3\n",
        "\n",
        "\n",
        "class EscalationAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"escalation\")\n",
        "\n",
        "    def check_escalation(self, score: int) -> Tuple[bool, str]:\n",
        "        if score < 3:\n",
        "            return True, \"Escalating to human support.\"\n",
        "        return False, \"No escalation needed.\"\n"
      ],
      "metadata": {
        "id": "6I6nJ96huzY3"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Session Memory"
      ],
      "metadata": {
        "id": "yo3pZ-2lAMoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù Session Memory and Conversation Tracking\n",
        "\n",
        "This part implements lightweight memory storage for tracking conversation history within a session.\n",
        "\n",
        "Session memory allows the orchestrator to maintain context across multiple user inputs, ensuring more coherent and consistent responses."
      ],
      "metadata": {
        "id": "s6488WNt-oxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Session Memory\n",
        "\n",
        "class SessionMemory:\n",
        "    def __init__(self, max_turns: int = 10):\n",
        "        self.max_turns = max_turns\n",
        "        self.sessions: Dict[str, List[Dict[str, str]]] = {}\n",
        "\n",
        "    def append_message(self, session_id: str, role: str, text: str):\n",
        "        if session_id not in self.sessions:\n",
        "            self.sessions[session_id] = []\n",
        "        self.sessions[session_id].append({\"role\": role, \"text\": text})\n",
        "        if len(self.sessions[session_id]) > self.max_turns:\n",
        "            self.sessions[session_id] = self.sessions[session_id][-self.max_turns:]\n",
        "\n",
        "    def get_context(self, session_id: str, window: Optional[int] = None) -> List[Dict[str, str]]:\n",
        "        ctx = self.sessions.get(session_id, [])\n",
        "        if window:\n",
        "            return ctx[-window:]\n",
        "        return ctx"
      ],
      "metadata": {
        "id": "HZCvtWt7u-Ik"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Orchestrator"
      ],
      "metadata": {
        "id": "rF4qsPt7AGw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù Central Orchestrator for Coordinating the AI Pipeline\n",
        "\n",
        "The orchestrator controls the full lifecycle of a user request by invoking all agents in the correct sequence.\n",
        "\n",
        "It handles:\n",
        "\n",
        "1. intent detection\n",
        "\n",
        "2. context retrieval\n",
        "\n",
        "3. response generation\n",
        "\n",
        "4. evaluation\n",
        "\n",
        "5. escalation decisions\n",
        "\n",
        "It returns a fully structured result that is ready to present to the user."
      ],
      "metadata": {
        "id": "aqbYQ9-k-8zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Orchestrator\n",
        "class Orchestrator(Agent):\n",
        "    def __init__(self, intent_agent: IntentClassifierAgent, retriever_agent: RetrieverAgent, generator_agent: ResponseGeneratorAgent,\n",
        "                 evaluator_agent: EvaluationAgent, escalation_agent: EscalationAgent,\n",
        "                 session_service: Optional[SessionMemory] = None, metrics: Optional[Dict[str, Any]] = None,\n",
        "                 llm_client: Optional[LLMClient] = None): # Added llm_client parameter\n",
        "        super().__init__(\"orchestrator\")\n",
        "        self.intent_agent = intent_agent\n",
        "        self.retriever_agent = retriever_agent\n",
        "        self.generator_agent = generator_agent\n",
        "        self.evaluator_agent = evaluator_agent\n",
        "        self.escalation_agent = escalation_agent\n",
        "        self.session_service = session_service\n",
        "        self.metrics = metrics\n",
        "\n",
        "        # All agents should use the same LLM client for consistency and to avoid re-initializing\n",
        "        self.llm = llm_client # Use the passed llm_client\n",
        "        if self.llm: # Only assign if llm is present\n",
        "            self.generator_agent.llm = self.llm\n",
        "            self.evaluator_agent.llm = self.llm\n",
        "\n",
        "    def _create_ticket(self, session_id: str, user_message: str, assistant_response: str) -> Dict[str, Any]:\n",
        "        self.warn(\"Creating support ticket\", session_id=session_id, user_message=user_message)\n",
        "        # Placeholder for actual ticket creation logic\n",
        "        return {\"ticket_id\": f\"TICKET-{hash(session_id + user_message)}\", \"status\": \"open\"}\n",
        "\n",
        "    def process_message(self, user_message: str, session_id: str = \"default_session\") -> Dict[str, Any]:\n",
        "        trace = {\"user_message\": user_message, \"session_id\": session_id, \"started_at\": time.time()}\n",
        "\n",
        "        if self.session_service:\n",
        "            self.session_service.append_message(session_id, \"user\", user_message)\n",
        "            context_messages = self.session_service.get_context(session_id)\n",
        "            trace[\"context_messages\"] = context_messages\n",
        "\n",
        "        # Classifier\n",
        "        intent = self.intent_agent.classify_intent(user_message)\n",
        "        trace[\"intent\"] = intent\n",
        "\n",
        "        # Retrieval\n",
        "        retrieved_chunks = self.retriever_agent.retrieve(user_message)\n",
        "        trace[\"retrieved_chunks\"] = retrieved_chunks\n",
        "\n",
        "        # Generation\n",
        "        gen_out = self.generator_agent.generate_answer(intent, user_message, retrieved_chunks)\n",
        "        trace[\"generated\"] = {\"text\": gen_out}\n",
        "\n",
        "        # Evaluation\n",
        "        try:\n",
        "            eval_score = self.evaluator_agent.evaluate(user_message, gen_out)\n",
        "        except Exception as e:\n",
        "            self.error(\"Evaluation error\", error=str(e))\n",
        "            eval_score = 3\n",
        "        trace[\"evaluation\"] = {\"score\": eval_score}\n",
        "\n",
        "        # Escalation\n",
        "        try:\n",
        "            escalate_flag, escalate_note = self.escalation_agent.check_escalation(eval_score)\n",
        "        except Exception as e:\n",
        "            self.error(\"Escalation check failed\", error=str(e))\n",
        "            escalate_flag = False\n",
        "            escalate_note = str(e)\n",
        "        trace[\"escalation\"] = {\"escalate\": bool(escalate_flag), \"note\": escalate_note}\n",
        "\n",
        "        if escalate_flag:\n",
        "            ticket = self._create_ticket(session_id, user_message, gen_out)\n",
        "            trace[\"escalation\"][\"ticket\"] = ticket\n",
        "\n",
        "        if self.session_service:\n",
        "            try:\n",
        "                self.session_service.append_message(session_id, \"assistant\", gen_out)\n",
        "            except Exception as e:\n",
        "                self.error(\"Failed to append assistant message to session\", error=str(e))\n",
        "\n",
        "        if self.metrics:\n",
        "            try:\n",
        "                if isinstance(self.metrics, dict):\n",
        "                    # simple metrics update\n",
        "                    self.metrics[\"total_messages\"] = self.metrics.get(\"total_messages\", 0) + 1\n",
        "                    self.metrics[\"avg_eval_score\"] = ((self.metrics.get(\"avg_eval_score\", 0) * (self.metrics.get(\"total_messages\", 1)-1)) + eval_score) / max(1, self.metrics.get(\"total_messages\", 1))\n",
        "                    if escalate_flag:\n",
        "                        self.metrics[\"total_escalations\"] = self.metrics.get(\"total_escalations\", 0) + 1\n",
        "            except Exception as e:\n",
        "                self.error(\"Metrics update failed\", error=str(e))\n",
        "\n",
        "        trace[\"completed_at\"] = time.time()\n",
        "        return trace"
      ],
      "metadata": {
        "id": "p6LK5Dm_wQ3n"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Example KB & Index Build"
      ],
      "metadata": {
        "id": "kFmOCt00_-KY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù Sample Knowledge Base and Vector Index Construction\n",
        "\n",
        "In this section, we include a simple example knowledge base and build the FAISS vector index.\n",
        "\n",
        "This ensures that the file can run end-to-end without external data dependencies and demonstrates the system workflow in a self-contained setup."
      ],
      "metadata": {
        "id": "80skEf77_UU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example KB and index build (so file runs end-to-end locally)\n",
        "\n",
        "KB_DOCS = [\n",
        "\"SaaS product with subscription billing, role-based access, usage analytics.\",\n",
        "\"User onboarding flow with health checks and email verification.\",\n",
        "\"Support escalation process with L1 ‚Üí L2 routing.\",\n",
        "\"Refunds are issued within 7-14 business days after approval.\"\n",
        "]\n",
        "\n",
        "emb_matrix, kb_chunks = build_kb_embeddings(KB_DOCS)\n",
        "faiss_index = build_faiss_index(emb_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Ga-VqtYcwRfK",
        "outputId": "7f65452d-d855-4103-a5d5-c2094ba7cb02"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 4\n",
            "FAISS index built with 4 vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Instantiate Agents & Orchestrator"
      ],
      "metadata": {
        "id": "VLVTW37K_1ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù Initializing All Agents and the System Orchestrator\n",
        "\n",
        "Here, we create instances of all previously defined agents and assemble them into the orchestrator.\n",
        "\n",
        "This prepares the entire support desk pipeline for real-time interaction through the Gradio interface."
      ],
      "metadata": {
        "id": "KlWVYURt_VgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate agents & orchestrator\n",
        "\n",
        "intent_agent = IntentClassifierAgent()\n",
        "retriever_agent = RetrieverAgent(index=faiss_index, chunks=kb_chunks)\n",
        "generator_agent = ResponseGeneratorAgent()\n",
        "evaluator_agent = EvaluationAgent()\n",
        "escalation_agent = EscalationAgent()\n",
        "\n",
        "session_service = SessionMemory(max_turns=6)\n",
        "metrics = {\"total_messages\": 0, \"avg_eval_score\": 0.0, \"total_escalations\": 0}\n",
        "\n",
        "# Attach a proper LLM client if API key is present; otherwise agents keep heuristic fallbacks\n",
        "llm_client = LLMClient(model=\"gemini-1.5-flash\") if API_KEY else None\n",
        "orch = Orchestrator(intent_agent, retriever_agent, generator_agent, evaluator_agent, escalation_agent, session_service=session_service, metrics=metrics, llm_client=llm_client)"
      ],
      "metadata": {
        "id": "6V06NKuIwzMX"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Gradio UI"
      ],
      "metadata": {
        "id": "8wmK4Mfv_y_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù Interactive Gradio Interface for User Interaction\n",
        "\n",
        "This final cell builds and launches the Gradio web interface.\n",
        "\n",
        "The UI enables users to type queries and view structured outputs including:\n",
        "\n",
        "- intent category\n",
        "\n",
        "- retrieved knowledge base snippets\n",
        "\n",
        "- AI-generated response\n",
        "\n",
        "- evaluation score\n",
        "\n",
        "- escalation decision\n",
        "\n",
        "This delivers a complete, interactive customer support experience."
      ],
      "metadata": {
        "id": "4Poonpx5_WWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio UI (simple)\n",
        "\n",
        "def run_orchestrator_sync(user_message: str):\n",
        "    try:\n",
        "        trace = orch.process_message(user_message, session_id=\"gradio-user\")\n",
        "        return trace\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "custom_css = \"\"\"\n",
        "#title {text-align:center; font-size: 28px; font-weight: bold;}\n",
        "#inner-box {background:#f7f7f7;padding:20px;border-radius:12px}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as demo:\n",
        "    gr.HTML(\"<h2 id='title'>ü§ñ AI Customer Support Desk</h2>\")\n",
        "    with gr.Column(elem_id=\"inner-box\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                user_input = gr.Textbox(label=\"Enter message\", placeholder=\"I was charged twice...\", lines=3)\n",
        "                run_btn = gr.Button(\"Run Assistant\")\n",
        "            with gr.Column(scale=1):\n",
        "                output_box = gr.JSON(label=\"Pipeline Trace\")\n",
        "        run_btn.click(fn=run_orchestrator_sync, inputs=user_input, outputs=output_box)\n",
        "\n",
        "# Demo launch is guarded so importing this module won't auto-launch in some environments\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "xWF3ssN0w5aB",
        "outputId": "68940901-0392-429b-ca67-3eaf0839d9b1"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://90db6319de841a27a6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://90db6319de841a27a6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[retriever][INFO] Call succeeded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.09ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[generator][INFO] Call succeeded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 380.94ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[evaluator][INFO] Call succeeded\n",
            "[retriever][INFO] Call succeeded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.92ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[generator][INFO] Call succeeded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 355.57ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[evaluator][INFO] Call succeeded\n",
            "[retriever][INFO] Call succeeded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 359.33ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[generator][INFO] Call succeeded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.42ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[evaluator][INFO] Call succeeded\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://84550b561d18f959b1.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://90db6319de841a27a6.gradio.live\n"
          ]
        }
      ]
    }
  ]
}